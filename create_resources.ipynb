{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Azure Resources\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.mgmt.resource import ResourceManagementClient\n",
    "from azure.common.credentials import ServicePrincipalCredentials\n",
    "from azure.mgmt.resource.resources.models import DeploymentMode\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start with specifying your subscription and resource group information and log in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subscription & resource group\n",
    "SUBSCRIPTION_NAME = ''\n",
    "RESOURCE_GROUP = ''\n",
    "LOCATION = 'eastus'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Login to subscription \n",
    "!az login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select subscription\n",
    "!az account set -s \"{SUBSCRIPTION_NAME}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get susbcription info\n",
    "temp = '\"az account show -s \\\\\"{}\\\\\"\"'.format(SUBSCRIPTION_NAME)\n",
    "subscription_id, tenant_id  =!eval {temp} | jq -r '.id, .tenantId'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create resource group\n",
    "!az group create -l {LOCATION} -n {RESOURCE_GROUP}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will also need service principal credentials for authentication. The following command creates and retrieves the credentials. For more information on service principal, check the docuemntation [here](https://docs.microsoft.com/en-us/cli/azure/create-an-azure-service-principal-azure-cli?view=azure-cli-latest)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and get service principal credentials\n",
    "temp = !az ad sp create-for-rbac | jq -r '.appId, .password'\n",
    "sp_client, sp_secret = temp[-2:] # filter role assignment warnings that are returned by 'az ad sp create-for-rbac'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following are parameters needed to create and access the main Azure resources. These include: Azure Container Registry (ACR), Batch AI, Blob Storage, and Logic Apps parameters. \n",
    "\n",
    "You can use the default values below as is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ACR\n",
    "ACR_NAME = 'baimmacr'\n",
    "ACR_SERVER = 'baimmacr.azurecr.io'\n",
    "BAI_DOCKER_IMG = 'baimmacr.azurecr.io/baimmimg:v1'\n",
    "SCHED_DOCKER_IMG = 'baimmacr.azurecr.io/baimmschedimg:v1'\n",
    "\n",
    "# Batch AI\n",
    "BAI_CLUSTER_NAME = 'baimmcluster'\n",
    "BAI_WORKSPACE = 'baimmws'\n",
    "BAI_USER = 'baimmuser'\n",
    "BAI_PASS = 'baimmpass'\n",
    "BAI_VM_SIZE = 'Standard_D2'\n",
    "BAI_VM_IMG = 'UbuntuLTS'\n",
    "BAI_NODES_MIN = 0\n",
    "BAI_NODES_MAX = 3\n",
    "\n",
    "# Blob storage\n",
    "BFS_CONTAINER = 'bfs' # shared across Batch AI nodes under /mnt/batch/tasks/shared/LS_root/mounts/bfs\n",
    "BLOB_ACCOUNT = 'baimmstorage'\n",
    "MODELS_CONTAINER = 'models'\n",
    "PREDS_CONTAINER = 'preds'\n",
    "DATA_CONTAINER = 'data'\n",
    "DATA_BLOB = 'sensor_data.csv' # name of data file to be copied to blob storage\n",
    "\n",
    "# Logic App\n",
    "LA_ACI_CON = 'aci'\n",
    "LA_WORKFLOW = 'baimmscheduler'\n",
    "LA_ACI_CON_JSON = 'sched/api_con_template.json'\n",
    "LA_JSON = 'sched/logic_app_template.json'\n",
    "LA_ACI_CONTAINER_NAME = 'baimmschedcontainer'\n",
    "LA_ACI_CONTAINER_GROUP = 'baimmcontainergroup'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ACR\n",
    "!az acr create --resource-group {RESOURCE_GROUP} --name {ACR_NAME} --sku Basic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Blob storage account\n",
    "!az storage account create -n {BLOB_ACCOUNT} -g {RESOURCE_GROUP} -l {LOCATION}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve Blob storage key\n",
    "blob_key = !az storage account keys list -g {RESOURCE_GROUP} -n {BLOB_ACCOUNT} | jq -r .[0].value\n",
    "blob_key = blob_key[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create models, predictions and data containers\n",
    "!az storage container create -n {MODELS_CONTAINER} --account-key {blob_key} --account-name {BLOB_ACCOUNT}\n",
    "!az storage container create -n {PREDS_CONTAINER} --account-key {blob_key} --account-name {BLOB_ACCOUNT}\n",
    "!az storage container create -n {DATA_CONTAINER} --account-key {blob_key} --account-name {BLOB_ACCOUNT}\n",
    "!az storage container create -n {BFS_CONTAINER} --account-key {blob_key} --account-name {BLOB_ACCOUNT}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Batch AI cluster and set auto-scaling\n",
    "!az batchai workspace create -g {RESOURCE_GROUP} -n {BAI_WORKSPACE}\n",
    "!az batchai cluster create -g {RESOURCE_GROUP} -n {BAI_CLUSTER_NAME} -w {BAI_WORKSPACE} -s {BAI_VM_SIZE} -i {BAI_VM_IMG} --min {BAI_NODES_MIN} --max {BAI_NODES_MAX} -u {BAI_USER} -p {BAI_PASS} --storage-account-name {BLOB_ACCOUNT} --storage-account-key {blob_key} --bfs-name {BFS_CONTAINER}\n",
    "!az batchai cluster auto-scale -g {RESOURCE_GROUP} -w {BAI_WORKSPACE} -n {BAI_CLUSTER_NAME} --min {BAI_NODES_MIN} --max {BAI_NODES_MAX}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main scoring Python script requires some config parameters to access the created Azure resources. We can generate those in the following cell and save the config file in json format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create config file for scoring script (predict.py)\n",
    "score_config = {\"blob_account\": BLOB_ACCOUNT,\n",
    "                \"blob_key\": blob_key,\n",
    "                \"models_blob_container\": MODELS_CONTAINER,\n",
    "                \"data_blob_container\": DATA_CONTAINER,\n",
    "                \"data_blob\" : DATA_BLOB,\n",
    "                \"predictions_blob_container\": PREDS_CONTAINER}\n",
    "\n",
    "with open('batchai/predict_config.json', 'w') as f:\n",
    "    json.dump(score_config, f, indent=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the scoring script and its config in place, we create a docker image that Batch AI can use to execute scoring, and push that to ACR. The image is defined in a Dockerfile in the repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending build context to Docker daemon  8.623MB\n",
      "Step 1/8 : FROM continuumio/miniconda3\n",
      " ---> 1284db959d5d\n",
      "Step 2/8 : EXPOSE 3000\n",
      " ---> Using cache\n",
      " ---> 3d5fae5b0095\n",
      "Step 3/8 : RUN apt-get update -y\n",
      " ---> Using cache\n",
      " ---> ccf358629d64\n",
      "Step 4/8 : COPY batchai/requirements.txt /\n",
      " ---> Using cache\n",
      " ---> 00d0a541d1c2\n",
      "Step 5/8 : RUN pip install --upgrade pip\n",
      " ---> Using cache\n",
      " ---> 174df659b223\n",
      "Step 6/8 : RUN python3 -m pip install -r requirements.txt\n",
      " ---> Using cache\n",
      " ---> 515608cba2cd\n",
      "Step 7/8 : COPY batchai/predict.py /\n",
      " ---> ac1842051fbf\n",
      "Step 8/8 : COPY batchai/predict_config.json /\n",
      " ---> 0f36ee3eb72e\n",
      "Successfully built 0f36ee3eb72e\n",
      "Successfully tagged baimmacr.azurecr.io/baimmimg:v1\n"
     ]
    }
   ],
   "source": [
    "# Create Batch AI docker img\n",
    "!sudo docker build -f batchai/Dockerfile -t {BAI_DOCKER_IMG} ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate that the image was created\n",
    "!sudo docker images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Login Succeeded\n",
      "WARNING! Your password will be stored unencrypted in /home/bleik/.docker/config.json.\n",
      "Configure a credential helper to remove this warning. See\n",
      "https://docs.docker.com/engine/reference/commandline/login/#credentials-store\n",
      "\n",
      "The push refers to repository [baimmacr.azurecr.io/baimmimg]\n",
      "\n",
      "\u001b[1B115dd695: Preparing \n",
      "\u001b[1Ba4452865: Preparing \n",
      "\u001b[1Baf025037: Preparing \n",
      "\u001b[1B48e56ac4: Preparing \n",
      "\u001b[1B87a12f5f: Preparing \n",
      "\u001b[1B709cca82: Preparing \n",
      "\u001b[1B4729c572: Preparing \n",
      "\u001b[1B589f916c: Preparing \n",
      "\u001b[1Bed6965f1: Preparing \n",
      "\u001b[1B38eef542: Preparing \n",
      "\u001b[2B38eef542: Layer already exists 8kB\u001b[10A\u001b[1K\u001b[K\u001b[11A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[8A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[4A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[6A\u001b[1K\u001b[K\u001b[1A\u001b[1K\u001b[Kv1: digest: sha256:77fff6411723259d2ea2c771807c7b015261514d18697d7db49b4621e3c6a86b size: 2626\n"
     ]
    }
   ],
   "source": [
    "# Login to ACR and push docker image\n",
    "!sudo az acr login --name {ACR_NAME}\n",
    "!sudo docker push {BAI_DOCKER_IMG}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following commands copy the pre-trained models and sample data from this repo to blob storage so that Batch AI can access them during job submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy models from local dir to blob container\n",
    "!az storage blob upload-batch -d {MODELS_CONTAINER} -s models --account-name {BLOB_ACCOUNT}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy dataset to blob\n",
    "!az storage blob upload -c {DATA_CONTAINER} -f data/'{DATA_BLOB}' -n '{DATA_BLOB}' --account-name {BLOB_ACCOUNT}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also generate a json config file for the Python script that creates and submits Batch AI jobs. The config file includes Batch AI, ACR, and service principal parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable ACR admin account authentication\n",
    "!az acr update -n {ACR_NAME} --admin-enabled true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get ACR's password (user is {ACR_NAME})\n",
    "acr_password = !az acr credential show --name {ACR_NAME} | jq -r .passwords[0].value\n",
    "acr_password = acr_password[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create config file for Batch AI job submission script (submit_jobs.py)\n",
    "submit_jobs_config = {\n",
    "  \"sp_tenant\": tenant_id,\n",
    "  \"sp_client\": sp_client,\n",
    "  \"sp_secret\": sp_secret,\n",
    "  \"resource_group_name\": RESOURCE_GROUP,\n",
    "  \"subscription_id\": subscription_id,\n",
    "  \"work_space\": BAI_WORKSPACE,\n",
    "  \"experiment_name\": \"baimm_score\",\n",
    "  \"cluster_name\": BAI_CLUSTER_NAME,\n",
    "  \"location\": LOCATION,\n",
    "  \"acr_server\": ACR_SERVER,\n",
    "  \"acr_image\": BAI_DOCKER_IMG,\n",
    "  \"acr_user\": ACR_NAME,\n",
    "  \"acr_password\": acr_password,\n",
    "  \"command_line\": \"python /predict.py {0} {1} {2}\",\n",
    "  \"std_out_err_path_prefix\": '/mnt/batch/tasks/shared/LS_root/mounts/{}'.format(BFS_CONTAINER),\n",
    "  \"config_file_path\": \"/predict_config.json\",\n",
    "  \"node_count\": 2,\n",
    "  \"device_ids\": [ 1, 2, 3 ],\n",
    "  \"tags\": [ 1, 2, 3, 4, 5 ],\n",
    "  \"job_name\": \"baimm_predict{0}_{1}\" # job name template\n",
    "}\n",
    "\n",
    "with open('sched/bai_pred_config.json', 'w') as f:\n",
    "    json.dump(submit_jobs_config, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will submit Batch AI jobs on a schedule defined and triggered by a Logic App. The Logic App creates a container instance from ACR and runs a Docker container that executes the job submission. That Docker image can be created and pushed to ACR using the following commands. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending build context to Docker daemon  8.623MB\n",
      "Step 1/9 : FROM continuumio/miniconda3\n",
      " ---> 1284db959d5d\n",
      "Step 2/9 : EXPOSE 3000\n",
      " ---> Using cache\n",
      " ---> 3d5fae5b0095\n",
      "Step 3/9 : RUN apt-get update -y\n",
      " ---> Using cache\n",
      " ---> ccf358629d64\n",
      "Step 4/9 : COPY sched/requirements.txt /\n",
      " ---> Using cache\n",
      " ---> 7d569bba9145\n",
      "Step 5/9 : RUN pip install --upgrade pip\n",
      " ---> Using cache\n",
      " ---> d9ec2292b0a2\n",
      "Step 6/9 : RUN python3 -m pip install -r requirements.txt\n",
      " ---> Using cache\n",
      " ---> b0c8b7b12086\n",
      "Step 7/9 : COPY sched/submit_jobs.py /\n",
      " ---> Using cache\n",
      " ---> a990caa88735\n",
      "Step 8/9 : COPY sched/bai_pred_config.json /\n",
      " ---> a0ac36c54ad0\n",
      "Step 9/9 : CMD python submit_jobs.py bai_pred_config.json\n",
      " ---> Running in 6278fd2d0ad9\n",
      "Removing intermediate container 6278fd2d0ad9\n",
      " ---> 824c994891bb\n",
      "Successfully built 824c994891bb\n",
      "Successfully tagged baimmacr.azurecr.io/baimmschedimg:v1\n"
     ]
    }
   ],
   "source": [
    "# Create scheduling docker img\n",
    "!sudo docker build -f sched/Dockerfile -t {SCHED_DOCKER_IMG} ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Login Succeeded\n",
      "WARNING! Your password will be stored unencrypted in /home/bleik/.docker/config.json.\n",
      "Configure a credential helper to remove this warning. See\n",
      "https://docs.docker.com/engine/reference/commandline/login/#credentials-store\n",
      "\n",
      "The push refers to repository [baimmacr.azurecr.io/baimmschedimg]\n",
      "\n",
      "\u001b[1Bf0124f0f: Preparing \n",
      "\u001b[1Be7d3f0f9: Preparing \n",
      "\u001b[1B182ab77c: Preparing \n",
      "\u001b[1Bdb5d4f81: Preparing \n",
      "\u001b[1Befad35ce: Preparing \n",
      "\u001b[1B709cca82: Preparing \n",
      "\u001b[1B4729c572: Preparing \n",
      "\u001b[1B589f916c: Preparing \n",
      "\u001b[1Bed6965f1: Preparing \n",
      "\u001b[1B38eef542: Preparing \n",
      "\u001b[2B38eef542: Layer already exists K\u001b[11A\u001b[1K\u001b[K\u001b[10A\u001b[1K\u001b[K\u001b[9A\u001b[1K\u001b[K\u001b[5A\u001b[1K\u001b[K\u001b[11A\u001b[1K\u001b[K\u001b[3A\u001b[1K\u001b[K\u001b[8A\u001b[1K\u001b[K\u001b[6A\u001b[1K\u001b[K\u001b[4A\u001b[1K\u001b[K\u001b[1A\u001b[1K\u001b[K\u001b[2A\u001b[1K\u001b[Kv1: digest: sha256:456131c3d23c6f4799d0f2796cd1919dd1d41e8d941fe00bfc698900e97e58a2 size: 2626\n"
     ]
    }
   ],
   "source": [
    "# Login to ACR and push docker image\n",
    "!sudo az acr login --name {ACR_NAME}\n",
    "!sudo docker push {SCHED_DOCKER_IMG}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we create the Logic App that acts as a scheduler for this solution. The Logic App and its API connection to Azure Container Instances (ACI) are created using an Azure Resournce Management (ARM) client and corresponding json templates that are stored in the repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create credentials and ARM client\n",
    "credentials = ServicePrincipalCredentials(client_id=sp_client,\n",
    "                                          secret=sp_secret,\n",
    "                                          tenant=tenant_id)\n",
    "arm_client = ResourceManagementClient(credentials, subscription_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an ACI API connection\n",
    "with open('sched/api_con_template.json') as f:\n",
    "    aci_api_con_template = json.load(f)\n",
    "\n",
    "aci_api_con_params = {\"location\": {\"value\": LOCATION},\n",
    "                      \"name\": {\"value\": LA_ACI_CON},\n",
    "                      \"subscription_id\": {\"value\": subscription_id}\n",
    "                      }\n",
    "\n",
    "aci_api_con_props = {\n",
    "    'mode': DeploymentMode.incremental,\n",
    "    'template': aci_api_con_template,\n",
    "    'parameters': aci_api_con_params\n",
    "}\n",
    "\n",
    "arm_client.deployments.create_or_update(RESOURCE_GROUP, LA_ACI_CON, aci_api_con_props)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Logic App\n",
    "with open('sched/logic_app_template.json') as f:\n",
    "    logic_app_template = json.load(f)\n",
    "\n",
    "logic_app_params = {\"location\": {\"value\": LOCATION},\n",
    "                    \"resource_group\": { \"value\": RESOURCE_GROUP },\n",
    "                    \"name\": {\"value\": LA_WORKFLOW},\n",
    "                    \"subscription_id\": {\"value\": subscription_id},\n",
    "                    \"container_name\": { \"value\": LA_ACI_CONTAINER_NAME },\n",
    "                    \"container_group\": { \"value\": LA_ACI_CONTAINER_GROUP },\n",
    "                    \"image_name\": { \"value\": SCHED_DOCKER_IMG },\n",
    "                    \"acr_pass\": { \"value\": acr_password },\n",
    "                    \"acr_user\": { \"value\": ACR_NAME },\n",
    "                    \"acr_server\": { \"value\": ACR_SERVER },\n",
    "                    \"aci_connection_name\": { \"value\": LA_ACI_CON }\n",
    "                    }\n",
    "\n",
    "logic_app_props = {\n",
    "    'mode': DeploymentMode.incremental,\n",
    "    'template': logic_app_template,\n",
    "    'parameters': logic_app_params\n",
    "}\n",
    "\n",
    "arm_client.deployments.create_or_update(RESOURCE_GROUP, LA_WORKFLOW, logic_app_props)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
